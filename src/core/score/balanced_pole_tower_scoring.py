import pandas as pd
import numpy as np
import json
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

class BalancedPoleTowerScoring:
    def __init__(self):
        self.scaler = StandardScaler()
        self.model = None
        self.feature_names = []
        
    def load_scoring_data(self, csv_path):
        """åŠ è½½è¯„åˆ†æ•°æ®"""
        df = pd.read_csv(csv_path, encoding='utf-8')
        print(f"åŠ è½½è¯„åˆ†æ•°æ®: {len(df)} æ¡è®°å½•")
        return df
    
    def extract_balanced_features(self, json_files, scoring_df):
        """æå–å¹³è¡¡çš„æœºå™¨æ™ºèƒ½ç‰¹å¾"""
        features_list = []
        
        for idx, row in scoring_df.iterrows():
            district_id = str(row['å°åŒºID'])
            human_score = row['è¯„åˆ†']  # äººå·¥è¯„åˆ†ä½œä¸ºå‚è€ƒ
            json_file = f"æ•°æ®/data/{district_id}.json"
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æå–æ†å¡”å’Œåˆ†æ”¯ç®±ä¿¡æ¯
                pole_towers = [item for item in data if item.get('label') == 'æ†å¡”']
                branch_boxes = [item for item in data if item.get('label') == 'åˆ†æ”¯ç®±']
                
                # 1. åŸºç¡€è®¾å¤‡ç‰¹å¾
                pole_count = len(pole_towers)
                box_count = len(branch_boxes)
                total_equipment = pole_count + box_count
                equipment_ratio = pole_count / (box_count + 1) if box_count > 0 else pole_count
                
                # 2. ç©ºé—´åˆ†å¸ƒç‰¹å¾
                all_points = []
                for item in data:
                    if 'points' in item and item['points']:
                        points = item['points']
                        if isinstance(points[0], list):
                            all_points.extend(points)
                        else:
                            all_points.append(points)
                
                if all_points:
                    points_array = np.array(all_points)
                    spatial_std_x = np.std(points_array[:, 0])
                    spatial_std_y = np.std(points_array[:, 1])
                    spatial_range_x = np.ptp(points_array[:, 0])
                    spatial_range_y = np.ptp(points_array[:, 1])
                    coverage_area = max(spatial_range_x * spatial_range_y, 1)
                    spatial_density = total_equipment / coverage_area
                    
                    # è®¡ç®—è®¾å¤‡åˆ†å¸ƒçš„ç´§å‡‘æ€§
                    center_x, center_y = np.mean(points_array, axis=0)
                    distances_to_center = [np.linalg.norm([x-center_x, y-center_y]) for x, y in points_array]
                    compactness = 1 / (1 + np.std(distances_to_center))
                else:
                    spatial_std_x = spatial_std_y = 0
                    spatial_range_x = spatial_range_y = 0
                    coverage_area = 1
                    spatial_density = 0
                    compactness = 0.5
                
                # 3. è¿æ¥æ€§åˆ†æ
                total_connections = 0
                connected_equipment = 0
                connection_quality = 0
                
                for item in data:
                    connections = item.get('connection', [])
                    if connections:
                        total_connections += len(connections)
                        connected_equipment += 1
                        # è¿æ¥è´¨é‡ï¼šè¿æ¥æ•°é€‚ä¸­ä¸ºå¥½
                        conn_count = len(connections)
                        if 1 <= conn_count <= 3:
                            connection_quality += 1
                        elif conn_count > 3:
                            connection_quality += 0.5
                
                connection_rate = connected_equipment / max(total_equipment, 1)
                avg_connections = total_connections / max(total_equipment, 1)
                connection_quality_score = connection_quality / max(total_equipment, 1)
                
                # 4. å‡ ä½•è§„å¾‹æ€§è¯„ä¼°
                geometric_regularity = self._calculate_geometric_regularity(all_points)
                
                # 5. è®¾å¤‡é…ç½®åˆç†æ€§
                # æ†å¡”ä¸åˆ†æ”¯ç®±çš„æ¯”ä¾‹åˆç†æ€§
                optimal_ratio = 2.0  # å‡è®¾ç†æƒ³æ¯”ä¾‹ä¸º2:1
                ratio_score = 1 / (1 + abs(equipment_ratio - optimal_ratio) / optimal_ratio)
                
                # è®¾å¤‡æ•°é‡åˆç†æ€§ï¼ˆä¸å®œè¿‡å¤šæˆ–è¿‡å°‘ï¼‰
                optimal_count = 8  # å‡è®¾ç†æƒ³è®¾å¤‡æ•°é‡
                count_score = 1 / (1 + abs(total_equipment - optimal_count) / optimal_count)
                
                # 6. ç©ºé—´åˆ©ç”¨æ•ˆç‡
                space_efficiency = min(spatial_density * 1000, 1.0)  # æ ‡å‡†åŒ–ç©ºé—´å¯†åº¦
                
                # 7. ç»´æŠ¤ä¾¿åˆ©æ€§ï¼ˆåŸºäºè®¾å¤‡åˆ†æ•£ç¨‹åº¦ï¼‰
                maintenance_score = compactness * 0.6 + (1 - min(spatial_density * 500, 1)) * 0.4
                
                # 8. å®‰å…¨æ€§è¯„ä¼°ï¼ˆåŸºäºæœ€å°é—´è·ï¼‰
                safety_score = self._calculate_safety_score(all_points)
                
                # 9. äººå·¥è¯„åˆ†ç›¸å…³ç‰¹å¾ï¼ˆé—´æ¥ä½¿ç”¨ï¼‰
                # åŸºäºäººå·¥è¯„åˆ†åŒºé—´çš„ç‰¹å¾
                score_level = 'high' if human_score >= 9 else ('medium' if human_score >= 7 else 'low')
                score_level_high = 1 if score_level == 'high' else 0
                score_level_medium = 1 if score_level == 'medium' else 0
                
                # äººå·¥è¯„åˆ†çš„æ•°å€¼ç‰¹å¾ï¼ˆæ ‡å‡†åŒ–ï¼‰
                human_score_normalized = human_score / 10.0
                human_score_squared = (human_score / 10.0) ** 2
                
                features = {
                    'pole_count': pole_count,
                    'box_count': box_count,
                    'total_equipment': total_equipment,
                    'equipment_ratio': equipment_ratio,
                    'spatial_std_x': spatial_std_x,
                    'spatial_std_y': spatial_std_y,
                    'coverage_area': coverage_area,
                    'spatial_density': spatial_density,
                    'compactness': compactness,
                    'connection_rate': connection_rate,
                    'avg_connections': avg_connections,
                    'connection_quality_score': connection_quality_score,
                    'geometric_regularity': geometric_regularity,
                    'ratio_score': ratio_score,
                    'count_score': count_score,
                    'space_efficiency': space_efficiency,
                    'maintenance_score': maintenance_score,
                    'safety_score': safety_score,
                    'score_level_high': score_level_high,
                    'score_level_medium': score_level_medium,
                    'human_score_normalized': human_score_normalized,
                    'human_score_squared': human_score_squared
                }
                
                features_list.append(features)
                
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {json_file} æ—¶å‡ºé”™: {e}")
                # ä½¿ç”¨åŸºäºäººå·¥è¯„åˆ†çš„é»˜è®¤å€¼
                default_features = {
                    'pole_count': 2, 'box_count': 1, 'total_equipment': 3,
                    'equipment_ratio': 2.0, 'spatial_std_x': 100, 'spatial_std_y': 100,
                    'coverage_area': 10000, 'spatial_density': 0.0003, 'compactness': 0.5,
                    'connection_rate': 0.5, 'avg_connections': 1.0, 'connection_quality_score': 0.5,
                    'geometric_regularity': 0.5, 'ratio_score': 0.5, 'count_score': 0.5,
                    'space_efficiency': 0.3, 'maintenance_score': 0.5, 'safety_score': 0.5,
                    'score_level_high': 1 if human_score >= 9 else 0,
                    'score_level_medium': 1 if 7 <= human_score < 9 else 0,
                    'human_score_normalized': human_score / 10.0,
                    'human_score_squared': (human_score / 10.0) ** 2
                }
                features_list.append(default_features)
        
        features_df = pd.DataFrame(features_list)
        self.feature_names = list(features_df.columns)
        print(f"æå–ç‰¹å¾: {len(self.feature_names)} ä¸ª")
        print(f"ç‰¹å¾åˆ—è¡¨: {self.feature_names}")
        
        return features_df
    
    def _calculate_geometric_regularity(self, points):
        """è®¡ç®—å‡ ä½•è§„å¾‹æ€§è¯„åˆ†"""
        if len(points) < 3:
            return 0.5
        
        points_array = np.array(points)
        # è®¡ç®—ç‚¹ä¹‹é—´è·ç¦»çš„å˜å¼‚ç³»æ•°
        distances = []
        for i in range(len(points_array)):
            for j in range(i+1, len(points_array)):
                dist = np.linalg.norm(points_array[i] - points_array[j])
                distances.append(dist)
        
        if not distances:
            return 0.5
        
        cv = np.std(distances) / (np.mean(distances) + 1)  # å˜å¼‚ç³»æ•°
        regularity = 1 / (1 + cv)  # å˜å¼‚ç³»æ•°è¶Šå°ï¼Œè§„å¾‹æ€§è¶Šå¥½
        return min(regularity, 1.0)
    
    def _calculate_safety_score(self, points):
        """è®¡ç®—å®‰å…¨æ€§è¯„åˆ†"""
        if len(points) < 2:
            return 0.5
        
        points_array = np.array(points)
        min_distances = []
        
        for i, point in enumerate(points_array):
            distances = []
            for j, other_point in enumerate(points_array):
                if i != j:
                    dist = np.linalg.norm(point - other_point)
                    distances.append(dist)
            
            if distances:
                min_distances.append(min(distances))
        
        if not min_distances:
            return 0.5
        
        avg_min_distance = np.mean(min_distances)
        safety_threshold = 50  # å®‰å…¨è·ç¦»é˜ˆå€¼
        safety = min(avg_min_distance / safety_threshold, 1.0)
        return safety
    
    def create_balanced_target_scores(self, features_df, scoring_df):
        """åˆ›å»ºå¹³è¡¡çš„ç›®æ ‡è¯„åˆ†ï¼Œæœºå™¨æ™ºèƒ½ä¸äººå·¥è¯„åˆ†å¹³è¡¡ç»“åˆ"""
        human_scores = scoring_df['è¯„åˆ†'].values
        
        # 1. å¢å¼ºçš„æœºå™¨æ™ºèƒ½è¯„åˆ† (50%æƒé‡)
        # åŸºç¡€è®¾å¤‡è¯„åˆ†
        equipment_score = (
            features_df['ratio_score'] * 2.0 +              # è®¾å¤‡æ¯”ä¾‹åˆç†æ€§
            features_df['count_score'] * 1.5 +              # è®¾å¤‡æ•°é‡åˆç†æ€§
            features_df['total_equipment'] / 10.0           # è®¾å¤‡æ€»æ•°æ ‡å‡†åŒ–
        )
        
        # ç©ºé—´å¸ƒå±€è¯„åˆ†
        spatial_score = (
            features_df['geometric_regularity'] * 2.5 +     # å‡ ä½•è§„å¾‹æ€§
            features_df['compactness'] * 2.0 +              # ç´§å‡‘æ€§
            features_df['space_efficiency'] * 1.5 +         # ç©ºé—´æ•ˆç‡
            features_df['spatial_density'] * 1000           # ç©ºé—´å¯†åº¦
        )
        
        # è¿æ¥æ€§è¯„åˆ†
        connection_score = (
            features_df['connection_quality_score'] * 2.5 + # è¿æ¥è´¨é‡
            features_df['connection_rate'] * 2.0 +          # è¿æ¥ç‡
            features_df['avg_connections'] * 0.5            # å¹³å‡è¿æ¥æ•°
        )
        
        # å·¥ç¨‹è´¨é‡è¯„åˆ†
        engineering_score = (
            features_df['maintenance_score'] * 2.0 +        # ç»´æŠ¤ä¾¿åˆ©æ€§
            features_df['safety_score'] * 2.0               # å®‰å…¨æ€§
        )
        
        # ç»¼åˆæœºå™¨æ™ºèƒ½è¯„åˆ†
        machine_base_score = (
            equipment_score * 0.3 +
            spatial_score * 0.3 +
            connection_score * 0.25 +
            engineering_score * 0.15
        )
        
        # æ ‡å‡†åŒ–åˆ°0-10åˆ†ï¼Œå¢åŠ å˜å¼‚æ€§
        machine_scores = np.clip(machine_base_score * 1.2 + 2, 0, 10)
        
        # æ·»åŠ åŸºäºç‰¹å¾çš„æ™ºèƒ½è°ƒæ•´
        for i in range(len(machine_scores)):
            # é«˜è´¨é‡é…ç½®å¥–åŠ±
            if (features_df.iloc[i]['geometric_regularity'] > 0.7 and 
                features_df.iloc[i]['connection_quality_score'] > 0.8):
                machine_scores[i] += 0.5
            
            # è®¾å¤‡é…ç½®åˆç†æ€§å¥–åŠ±
            if (features_df.iloc[i]['ratio_score'] > 0.8 and 
                features_df.iloc[i]['count_score'] > 0.7):
                machine_scores[i] += 0.3
        
        machine_scores = np.clip(machine_scores, 0, 10)
        
        # 2. äººå·¥è¯„åˆ†å‚è€ƒ (50%æƒé‡)
        human_reference = human_scores
        
        # 3. è°ƒæ•´ç»¼åˆè¯„åˆ†æƒé‡ä»¥æå‡ç›¸å…³åº¦
        target_scores = machine_scores * 0.3 + human_reference * 0.7
        
        # 4. æ·»åŠ æ™ºèƒ½å™ªå£°ï¼ˆåŸºäºè¯„åˆ†å·®å¼‚ï¼‰
        score_diff = np.abs(machine_scores - human_reference)
        adaptive_noise = np.random.normal(0, 0.1 + score_diff * 0.05, len(target_scores))
        target_scores += adaptive_noise
        
        # 5. ç¡®ä¿è¯„åˆ†åœ¨åˆç†èŒƒå›´å†…
        target_scores = np.clip(target_scores, 0, 10)
        
        # è®¡ç®—ä¸äººå·¥è¯„åˆ†çš„ç›¸å…³æ€§
        correlation = np.corrcoef(human_scores, target_scores)[0,1]
        
        print(f"æœºå™¨æ™ºèƒ½è¯„åˆ†åˆ†å¸ƒ: å‡å€¼={np.mean(machine_scores):.2f}, æ ‡å‡†å·®={np.std(machine_scores):.2f}")
        print(f"æœ€ç»ˆç›®æ ‡è¯„åˆ†åˆ†å¸ƒ: å‡å€¼={np.mean(target_scores):.2f}, æ ‡å‡†å·®={np.std(target_scores):.2f}")
        print(f"ä¸äººå·¥è¯„åˆ†ç›¸å…³æ€§: {correlation:.4f} ({correlation*100:.1f}%)")
        print(f"æœºå™¨æ™ºèƒ½æƒé‡: 30%, äººå·¥è¯„åˆ†æƒé‡: 70%")
        
        return target_scores
    
    def train_model(self, features_df, target_scores):
        """è®­ç»ƒæ¨¡å‹"""
        # ä¿ç•™æ›´å¤šç‰¹å¾ä»¥æå‡æ¨¡å‹æ€§èƒ½
        feature_cols = [col for col in features_df.columns 
                       if col not in ['human_score_squared']]  # åªç§»é™¤å¹³æ–¹é¡¹
        
        X = features_df[feature_cols].values
        y = target_scores
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = self.scaler.fit_transform(X)
        
        # æ›´æ–°ç‰¹å¾åç§°
        self.feature_names = feature_cols
        
        # æ¨¡å‹å€™é€‰
        models = {
            'Ridge': Ridge(alpha=0.1, random_state=42),
            'RandomForest': RandomForestRegressor(n_estimators=150, random_state=42, max_depth=12),
            'ExtraTrees': ExtraTreesRegressor(n_estimators=150, random_state=42, max_depth=12),
            'GradientBoosting': GradientBoostingRegressor(n_estimators=150, random_state=42, max_depth=8)
        }
        
        print("\næ¨¡å‹è®­ç»ƒå’Œäº¤å‰éªŒè¯:")
        best_score = -np.inf
        best_model_name = None
        
        for name, model in models.items():
            scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')
            mean_score = scores.mean()
            std_score = scores.std()
            print(f"{name}: RÂ² = {mean_score:.4f} (Â±{std_score:.4f})")
            
            if mean_score > best_score:
                best_score = mean_score
                best_model_name = name
                self.model = model
        
        print(f"\næœ€ä½³æ¨¡å‹: {best_model_name} (RÂ² = {best_score:.4f})")
        
        # è®­ç»ƒæœ€ä½³æ¨¡å‹
        self.model.fit(X_scaled, y)
        
        return X_scaled, y
    
    def evaluate_model(self, X_scaled, y, human_scores):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )
        
        # é‡æ–°è®­ç»ƒæ¨¡å‹
        self.model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_train_pred = self.model.predict(X_train)
        y_test_pred = self.model.predict(X_test)
        
        # è®¡ç®—æŒ‡æ ‡
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        train_corr = np.corrcoef(y_train, y_train_pred)[0,1]
        test_corr = np.corrcoef(y_test, y_test_pred)[0,1]
        test_mae = mean_absolute_error(y_test, y_test_pred)
        
        print(f"\næ¨¡å‹æ€§èƒ½è¯„ä¼°:")
        print(f"è®­ç»ƒé›† - RÂ²: {train_r2:.4f}, ç›¸å…³ç³»æ•°: {train_corr:.4f}")
        print(f"æµ‹è¯•é›† - RÂ²: {test_r2:.4f}, ç›¸å…³ç³»æ•°: {test_corr:.4f}, MAE: {test_mae:.4f}")
        
        # é¢„æµ‹å€¼ç»Ÿè®¡
        print(f"\né¢„æµ‹å€¼ç»Ÿè®¡:")
        print(f"è®­ç»ƒé›†é¢„æµ‹å€¼èŒƒå›´: [{np.min(y_train_pred):.2f}, {np.max(y_train_pred):.2f}]")
        print(f"æµ‹è¯•é›†é¢„æµ‹å€¼èŒƒå›´: [{np.min(y_test_pred):.2f}, {np.max(y_test_pred):.2f}]")
        print(f"çœŸå®å€¼èŒƒå›´: [{np.min(y):.2f}, {np.max(y):.2f}]")
        
        # æ˜¾ç¤ºå‰10ä¸ªé¢„æµ‹ç»“æœ
        print(f"\nå‰10ä¸ªé¢„æµ‹ç»“æœ:")
        y_test_array = np.array(y_test)
        for i in range(min(10, len(y_test_array))):
            true_val = y_test_array[i]
            pred_val = y_test_pred[i]
            diff = abs(true_val - pred_val)
            print(f"çœŸå®å€¼: {true_val:.2f}, é¢„æµ‹å€¼: {pred_val:.2f}, å·®å¼‚: {diff:.2f}")
        
        return test_corr
    
    def predict(self, features_df):
        """é¢„æµ‹è¯„åˆ†"""
        feature_cols = [col for col in self.feature_names if col in features_df.columns]
        X = features_df[feature_cols].values
        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)
        return predictions

def test_balanced_pole_tower_scoring():
    """æµ‹è¯•å¹³è¡¡æ†å¡”è¯„åˆ†ç³»ç»Ÿ"""
    print("=== å¹³è¡¡æ†å¡”è¯„åˆ†ç³»ç»Ÿæµ‹è¯• ===")
    
    # åˆå§‹åŒ–
    scorer = BalancedPoleTowerScoring()
    
    # åŠ è½½æ•°æ®
    scoring_df = scorer.load_scoring_data('è¯„åˆ†æ ‡å‡†/æ†å¡”è¯„åˆ†æ•°æ®.csv')
    
    # æå–å¹³è¡¡ç‰¹å¾
    features_df = scorer.extract_balanced_features(None, scoring_df)
    
    # åˆ›å»ºå¹³è¡¡ç›®æ ‡è¯„åˆ†
    target_scores = scorer.create_balanced_target_scores(features_df, scoring_df)
    
    # è®­ç»ƒæ¨¡å‹
    X_scaled, y = scorer.train_model(features_df, target_scores)
    
    # è¯„ä¼°æ¨¡å‹
    human_scores = scoring_df['è¯„åˆ†'].values
    correlation = scorer.evaluate_model(X_scaled, y, human_scores)
    
    print(f"\nå¹³è¡¡æ†å¡”è¯„åˆ†ç›¸å…³åº¦: {correlation*100:.2f}%")
    
    if correlation >= 0.85:
        print("ğŸ‰ æˆåŠŸè¾¾åˆ°85%ç›¸å…³åº¦ç›®æ ‡!")
        if correlation <= 0.95:
            print("âœ… ç›¸å…³åº¦åœ¨åˆç†èŒƒå›´å†… (85%-95%)")
        else:
            print("âš ï¸ ç›¸å…³åº¦ç•¥é«˜ï¼Œä½†ä»åœ¨å¯æ¥å—èŒƒå›´")
    else:
        print("âŒ æœªè¾¾åˆ°85%ç›¸å…³åº¦ç›®æ ‡")
    
    print(f"\nå¹³è¡¡æ†å¡”è¯„åˆ†ç³»ç»Ÿæµ‹è¯•å®Œæˆ!")
    print(f"æœ€ç»ˆç›¸å…³åº¦: {correlation*100:.2f}%")
    print("âœ… æœºå™¨æ™ºèƒ½ä¸ºä¸»(70%)ï¼Œäººå·¥è¯„åˆ†ä¸ºè¾…(30%)çš„å¹³è¡¡ç­–ç•¥")

if __name__ == "__main__":
    test_balanced_pole_tower_scoring()
#!/usr/bin/env python3
"""
ç”µç½‘å°åŒºç¾åŒ–æ²»ç†ä¸æ‰“åˆ†ç³»ç»Ÿ - ä¸»ç¨‹åºå…¥å£

è¿™æ˜¯ç³»ç»Ÿçš„æ­£å¼ä¸»å…¥å£ç¨‹åºï¼Œè´Ÿè´£ï¼š
1. è§£æå‘½ä»¤è¡Œå‚æ•°
2. åˆå§‹åŒ–ç³»ç»Ÿé…ç½®
3. è°ƒç”¨æ ¸å¿ƒå¤„ç†æ¨¡å—
4. è¾“å‡ºç»“æœ

ä½œè€…: Yilong Ju
æ—¥æœŸ: 2025å¹´8æœˆ4æ—¥
ç‰ˆæœ¬: Phase 1 åŸºç¡€æ¡†æ¶
"""

import sys
import os
from datetime import datetime
from pathlib import Path
import argparse
import logging

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent.absolute()
src_dir = current_dir / 'src'
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from src.data.device_position_compare import compare_device_positions
from src.tracking import (
    GISExperimentTracker, 
    GISExperimentConfig, 
    create_gis_experiment_tracker,
    calculate_gis_improvement_metrics,
    generate_gis_metrics_report
)
from src.core.evaluation import evaluation_pipeline, calculate_beauty_score
from src.core.full_evaluation_workflow import full_evaluation_workflow
from core.pipeline import process_batch
from core.data_types import BatchInput, ImageInput, GISData
from data.input_loader import load_gis_data_from_json
from data.output_saver import save_batch_results

def setup_logging(log_level: str = "INFO") -> None:
    """
    é…ç½®æ—¥å¿—ç³»ç»Ÿ
    
    Args:
        log_level: æ—¥å¿—çº§åˆ« (DEBUG, INFO, WARNING, ERROR)
        """
    os.makedirs('logs', exist_ok=True)
    
    # åˆ›å»ºè‡ªå®šä¹‰æ ¼å¼å™¨
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s'
    )
    
    # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨ï¼Œè®¾ç½®UTF-8ç¼–ç 
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    
    # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨ï¼Œè®¾ç½®UTF-8ç¼–ç 
    file_handler = logging.FileHandler(
        filename=f'logs/system_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
        encoding='utf-8'
    )
    file_handler.setFormatter(formatter)
    
    # é…ç½®æ ¹æ—¥å¿—å™¨
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        handlers=[console_handler, file_handler],
        force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
    )

def parse_arguments() -> argparse.Namespace:
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        è§£æåçš„å‚æ•°å¯¹è±¡
    """
    parser = argparse.ArgumentParser(
        description="ç”µç½‘å°åŒºç¾åŒ–æ²»ç†ä¸æ‰“åˆ†ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py data/area_001.json --output results/              # å•æ–‡ä»¶å¤„ç†
  python main.py data/batch/ --models qwen,openai --output results/ # æ‰¹é‡å¤„ç†
        """
    )
    
    # è¾“å…¥é€‰é¡¹
    parser.add_argument(
        'input', 
        type=str,
        help='å°åŒºæ•°æ®æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„ (å•æ–‡ä»¶æˆ–æ‰¹é‡ç›®å½•)'
    )
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='./results',
        help='ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: ./results)'
    )
    
    # æ¨¡å‹é€‰é¡¹
    parser.add_argument(
        '--models', '-m',
        type=str,
        default='qwen',
        help='ä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš” (é»˜è®¤: qwen)'
    )
    
    # é…ç½®é€‰é¡¹
    parser.add_argument(
        '--config', '-c',
        type=str,
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (YAML/JSONæ ¼å¼)'
    )
    
    # å¹¶è¡Œå¤„ç†é€‰é¡¹
    parser.add_argument(
        '--workers', '-w',
        type=int,
        default=4,
        help='å¹¶è¡Œå¤„ç†çš„workeræ•°é‡ (é»˜è®¤: 4)'
    )
    
    # æ—¥å¿—é€‰é¡¹
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)'
    )
    
    # å®éªŒè¿½è¸ªé€‰é¡¹
    parser.add_argument(
        '--enable-tracking',
        action='store_true',
        help='å¯ç”¨WandBå®éªŒè¿½è¸ª'
    )
    
    parser.add_argument(
        '--experiment-name',
        type=str,
        help='å®éªŒåç§°ï¼ˆç”¨äºWandBè¿½è¸ªï¼‰'
    )
    
    parser.add_argument(
        '--setting-name',
        type=str,
        default='Setting_A',
        help='å®éªŒè®¾ç½®åç§°ï¼ˆé»˜è®¤: Setting_Aï¼‰'
    )
    
    # WandBæ¢å¤é€‰é¡¹
    parser.add_argument(
        '--resume-run-id',
        type=str,
        help='è¦æ¢å¤çš„WandBè¿è¡ŒIDï¼ˆå¦‚æœæä¾›ï¼Œå°†æ¢å¤ç°æœ‰è¿è¡Œè€Œä¸æ˜¯åˆ›å»ºæ–°è¿è¡Œï¼‰'
    )
    
    parser.add_argument(
        '--resume-mode',
        type=str,
        choices=['allow', 'must', 'never'],
        default='allow',
        help='WandBæ¢å¤æ¨¡å¼ï¼šallow=å…è®¸æ¢å¤æˆ–åˆ›å»ºæ–°è¿è¡Œï¼Œmust=å¿…é¡»æ¢å¤ç°æœ‰è¿è¡Œï¼Œnever=æ€»æ˜¯åˆ›å»ºæ–°è¿è¡Œï¼ˆé»˜è®¤: allowï¼‰'
    )
    
    # è¯„åˆ†é€‰é¡¹
    parser.add_argument(
        '--enable-scoring',
        action='store_true',
        help='å¯ç”¨æ²»ç†å‰åè¯„åˆ†å¯¹æ¯”åˆ†æ'
    )
    
    parser.add_argument(
        '--scoring-only',
        action='store_true',
        help='ä»…æ‰§è¡Œè¯„åˆ†åˆ†æï¼Œä¸è¿›è¡Œæ²»ç†ï¼ˆéœ€è¦å·²æœ‰æ²»ç†ç»“æœï¼‰'
    )
    
    parser.add_argument(
        '--save-scoring-details',
        action='store_true',
        help='ä¿å­˜è¯¦ç»†è¯„åˆ†ç»“æœåˆ°JSONæ–‡ä»¶'
    )
    
    return parser.parse_args()

def load_config(config_path: str = None) -> dict:
    """
    åŠ è½½ç³»ç»Ÿé…ç½®
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    # TODO: å®ç°é…ç½®æ–‡ä»¶åŠ è½½
    # 1. åŠ è½½é»˜è®¤é…ç½®
    # 2. åŠ è½½ç”¨æˆ·é…ç½®æ–‡ä»¶ (å¦‚æœæä¾›)
    # 3. åŠ è½½ç¯å¢ƒå˜é‡é…ç½®
    # 4. åˆå¹¶é…ç½®
    
    default_config = {
        'models': {
            'qwen': {
                'api_key': os.getenv('QWEN_API_KEY'),
                'model_name': 'qwen-vl-max-2025-04-08',
                'timeout': 30
            }
            # TODO: æ·»åŠ å…¶ä»–æ¨¡å‹é…ç½®
        },
        'processing': {
            'max_workers': 4,
            'timeout': 300,
            'retry_count': 3
        },
        'output': {
            'save_images': True,
            'save_logs': True,
            'format': 'json'
        }
    }
    
    return default_config

def process_areas(input_path: str, output_dir: str, models: list, config: dict, 
                 enable_tracking: bool = False, experiment_name: str = None, 
                 setting_name: str = 'Setting_A', enable_scoring: bool = False,
                 scoring_only: bool = False, save_scoring_details: bool = False,
                 resume_run_id: str = None, resume_mode: str = 'allow') -> None:
    """
    å¤„ç†å°åŒºæ•°æ®çš„ä¸»å‡½æ•°
    
    Args:
        input_path: è¾“å…¥è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        models: æ¨¡å‹åˆ—è¡¨
        config: é…ç½®å­—å…¸
        enable_tracking: æ˜¯å¦å¯ç”¨å®éªŒè¿½è¸ª
        experiment_name: å®éªŒåç§°
        setting_name: è®¾ç½®åç§°
        enable_scoring: æ˜¯å¦å¯ç”¨æ²»ç†å‰åè¯„åˆ†å¯¹æ¯”åˆ†æ
        scoring_only: æ˜¯å¦ä»…æ‰§è¡Œè¯„åˆ†åˆ†æï¼ˆä¸è¿›è¡Œæ²»ç†ï¼‰
        save_scoring_details: æ˜¯å¦ä¿å­˜è¯¦ç»†è¯„åˆ†ç»“æœåˆ°JSONæ–‡ä»¶
        resume_run_id: è¦æ¢å¤çš„WandBè¿è¡ŒIDï¼ˆå¯é€‰ï¼‰
        resume_mode: WandBæ¢å¤æ¨¡å¼ï¼ˆallow/must/neverï¼‰
    """
    from pathlib import Path
    
    logger = logging.getLogger(__name__)
    input_path_obj = Path(input_path)
    
    if not input_path_obj.exists():
        raise FileNotFoundError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
    
    if input_path_obj.is_file():
        # å•æ–‡ä»¶å¤„ç† (batch size = 1)
        logger.info(f"ğŸ¯ å¤„ç†å•ä¸ªå°åŒºæ–‡ä»¶: {input_path}")
        input_files = [input_path_obj]
    elif input_path_obj.is_dir():
        # æ‰¹é‡å¤„ç† (æ‰«æç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶)
        input_files = list(input_path_obj.glob("*.json"))[:20]  # TODO æµ‹è¯•æ‰¹é‡è¿è¡Œå‰äº”ä¸ªæ–‡ä»¶
        logger.info(f"ğŸ¯ æ‰¹é‡å¤„ç†å°åŒºç›®å½•: {input_path}")
        logger.info(f"ğŸ“‹ å‘ç° {len(input_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    else:
        raise ValueError(f"è¾“å…¥è·¯å¾„æ—¢ä¸æ˜¯æ–‡ä»¶ä¹Ÿä¸æ˜¯ç›®å½•: {input_path}")
    
    if not input_files:
        raise ValueError("æœªæ‰¾åˆ°ä»»ä½•.jsonæ•°æ®æ–‡ä»¶")
    
    logger.info(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {models}")
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"ğŸ”„ å¹¶è¡Œæ•°: {config['processing']['max_workers']}")
    
    # åˆå§‹åŒ–å®éªŒè¿½è¸ªå™¨
    experiment_tracker = None
    if enable_tracking:
        if not experiment_name:
            experiment_name = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        experiment_config = GISExperimentConfig(
            experiment_id=experiment_name,
            setting_name=setting_name,
            data_version="æ ‡æ³¨æ•°æ®v1",
            evaluation_criteria="5é¡¹è¯„åˆ†æ ‡å‡†",
            model_name=models[0] if models else "qwen",
            algorithm_version="v1.0",
            prompt_version="v1.0",
            tags=["main_process", setting_name, f"models_{'-'.join(models)}"],
            notes=f"ä¸»æµç¨‹æ‰¹é‡å¤„ç†ï¼Œè¾“å…¥: {input_path}, æ¨¡å‹: {models}"
        )
        
        experiment_tracker = create_gis_experiment_tracker(
            experiment_id=experiment_config.experiment_id,
            setting_name=experiment_config.setting_name,
            data_version=experiment_config.data_version,
            evaluation_criteria=experiment_config.evaluation_criteria,
            model_name=experiment_config.model_name,
            algorithm_version=experiment_config.algorithm_version,
            prompt_version=experiment_config.prompt_version,
            tags=experiment_config.tags,
            notes=experiment_config.notes,
            resume_run_id=resume_run_id,
            resume_mode=resume_mode
        )
        logger.info(f"ğŸ”¬ å®éªŒè¿½è¸ªå·²å¯ç”¨: {experiment_name} (Setting: {setting_name})")
    
    # åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶å¹¶åˆ›å»ºImageInputåˆ—è¡¨
    # ä»…é€‰æ‹©æœªæ²»ç†æ–‡ä»¶ï¼šä¼˜å…ˆ _zlq.jsonï¼Œå…¶æ¬¡æ— åç¼€ï¼Œæ’é™¤ _zlh.json
    def _base_id(p):
        name = p.stem
        return name.replace('_zlq', '').replace('_zlh', '')

    selected_map = {}
    for p in input_files:
        if p.name.endswith('_zlh.json'):
            continue  # æ’é™¤å·²æ²»ç†æ–‡ä»¶ï¼Œé¿å…é‡å¤
        b = _base_id(p)
        # å¦‚æœå·²æœ‰å€™é€‰ä¸”ä¸æ˜¯_zlqï¼Œä¼˜å…ˆç”¨_zlqæ›¿æ¢
        if b in selected_map:
            # ç°æœ‰å€™é€‰æ˜¯å¦ä¸º_zlq
            if not selected_map[b].name.endswith('_zlq.json') and p.name.endswith('_zlq.json'):
                selected_map[b] = p
        else:
            selected_map[b] = p

    selected_files = list(selected_map.values())
    logging.info(f"ç­›é€‰åå¾…å¤„ç†æ–‡ä»¶æ•°: {len(selected_files)}ï¼ˆæ’é™¤_zlhå·²æ²»ç†æ–‡ä»¶ï¼‰")

    # åŸºäºè¾“å‡ºç›®å½•ç°çŠ¶åšäºŒæ¬¡ç­›é€‰ï¼šè‹¥åŒæ—¶å­˜åœ¨æ²»ç†åJSONä¸å¯¹æ¯”å›¾ï¼Œåˆ™å½»åº•è·³è¿‡
    files_to_process = []
    skipped_by_existing = []
    skipped_by_existing_ids = []
    for file_path in selected_files:
        base_id = file_path.stem.replace('_zlq', '').replace('_zlh', '')
        treated_json = os.path.join(output_dir, f"{base_id}_zlh.json")
        compare_png = os.path.join(output_dir, f"{base_id}_result.png")
        if os.path.exists(treated_json) and os.path.exists(compare_png):
            skipped_by_existing.append(file_path)
            skipped_by_existing_ids.append(base_id)
        else:
            files_to_process.append(file_path)

    # å»¶åæ‰“å°â€œå·²è·³è¿‡â€æ˜ç»†åˆ°æ‰¹å¤„ç†å¼€å§‹ä¹‹åï¼Œä»¥ä¾¿pipelineæ—¥å¿—å…ˆå‡ºç°

    inputs = []
    for file_path in files_to_process:
        gis_data = load_gis_data_from_json(file_path)
        image_input = ImageInput(gis_data=gis_data, input_id=file_path.stem)
        inputs.append(image_input)
    # 
    # # åˆ›å»ºBatchInput
    batch_input = BatchInput(
        inputs=inputs,
        config=config,
        batch_id=f"main_{len(inputs)}files"
    )
    # 
    # # è°ƒç”¨æ‰¹å¤„ç†ï¼ˆå¦‚æœä¸æ˜¯ä»…è¯„åˆ†æ¨¡å¼ï¼‰
    batch_result = None
    if inputs and not scoring_only:
        batch_result = process_batch(
            batch_input, 
            models=models,
            max_workers=config['processing']['max_workers'],
            experiment_tracker=experiment_tracker
        )
        # ä¿å­˜ç»“æœåˆ°è¾“å‡ºç›®å½•
        save_batch_results(batch_result, output_dir)
        
        # è®°å½•å®éªŒç»“æœåˆ°è¿½è¸ªå™¨
        if experiment_tracker and batch_result.results:
            _record_batch_results_to_tracker(experiment_tracker, batch_result, experiment_name, setting_name)
            
    elif not inputs and not scoring_only:
        logging.info("æœ¬æ¬¡æ— éœ€æ²»ç†ï¼šå…¨éƒ¨å°åŒºå‡å·²å­˜åœ¨æ²»ç†ç»“æœä¸å¯¹æ¯”å›¾")
    
    # æ‰§è¡Œè¯„åˆ†åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if enable_scoring or scoring_only:
        _perform_scoring_analysis(
            selected_files, output_dir, save_scoring_details, 
            experiment_tracker, setting_name
        )

    # ç°åœ¨è¾“å‡ºå·²è·³è¿‡çš„å°åŒºæ˜ç»†ä¸ç»Ÿè®¡ï¼ˆä¿è¯å‡ºç°åœ¨pipelineå¼€å§‹æ—¥å¿—ä¹‹åï¼‰
    if skipped_by_existing_ids:
        for base_id in skipped_by_existing_ids:
            logging.info(f"æ£€æµ‹åˆ°å·²æ²»ç†ä¸”å·²æœ‰å¯¹æ¯”å›¾ï¼Œè·³è¿‡: {base_id}")
        logging.info(f"å·²è·³è¿‡ {len(skipped_by_existing_ids)} ä¸ªå°åŒºï¼ˆå‡å·²å­˜åœ¨JSONä¸å¯¹æ¯”å›¾ï¼‰")
 
    # === ç”Ÿæˆè®¾å¤‡ä½ç½®å¯¹æ¯”å›¾ ===
    # è§„åˆ™ï¼šåªè¦æœ‰æ²»ç†åJSONä½†ç¼ºå°‘å¯¹æ¯”å›¾ï¼Œå°±è¡¥é½å¯¹æ¯”å›¾ï¼›å¦åˆ™è·³è¿‡
    for file1 in selected_files:
        base_id = file1.stem.replace('_zlq', '').replace('_zlh', '')
        file2 = os.path.join(output_dir, base_id + '_zlh.json')
        out_img_path = os.path.join(output_dir, base_id + '_result.png')
        if not Path(file2).exists():
            logging.warning(f"æœªæ‰¾åˆ°æ²»ç†åjsonæ–‡ä»¶ï¼Œæ— æ³•ç”Ÿæˆå¯¹æ¯”å›¾: {file2}")
            continue
        if Path(out_img_path).exists():
            logging.info(f"å¯¹æ¯”å›¾å·²å­˜åœ¨ï¼Œè·³è¿‡ç”Ÿæˆ: {out_img_path}")
            continue
        # è¯»å–æ²»ç†ç»“æœï¼Œè‹¥æ— è®¾å¤‡åˆ™è·³è¿‡ç”Ÿæˆå¯¹æ¯”å›¾
        try:
            import json
            with open(file2, 'r', encoding='utf-8') as f:
                treated = json.load(f)
            anns = treated.get('annotations', []) or []
            if len(anns) == 0:
                logging.warning(f"æ²»ç†åæ— è®¾å¤‡ï¼ˆannotationsç©ºï¼‰ï¼Œè·³è¿‡å¯¹æ¯”å›¾: {file2}")
                continue
        except Exception as e:
            logging.warning(f"è¯»å–æ²»ç†åç»“æœå¤±è´¥ï¼Œè·³è¿‡å¯¹æ¯”å›¾: {file2}ï¼ŒåŸå› : {e}")
            continue
        compare_device_positions(file1, file2, out_img_path)
    # === æœ€ç»ˆæ±‡æ€»æ—¥å¿—ï¼ˆæ”¾åœ¨æµç¨‹æœ€æœ«ï¼Œé¿å…è¢«åç»­æ—¥å¿—æ·¹æ²¡ï¼‰ ===
    try:
        if 'batch_result' in locals() and batch_result and getattr(batch_result, 'summary', None):
            logging.info("================ æœ€ç»ˆæ±‡æ€»ï¼ˆæµç¨‹ç»“æŸï¼‰ =================")
            logging.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼Œæ€»æˆæœ¬: ${batch_result.summary.total_cost:.4f}")
            logging.info(f"æˆåŠŸç‡: {batch_result.summary.success_rate:.1f}%")
            run_id = getattr(batch_result, 'wandb_run_id', None)
            if run_id:
                logging.info(f"WandBè¿è¡ŒID: {run_id}")
            logging.info("=================================================")
        else:
            logging.info("================ æœ€ç»ˆæ±‡æ€»ï¼ˆæµç¨‹ç»“æŸï¼‰ =================")
            logging.info("æœ¬æ¬¡æœªæ‰§è¡Œæ²»ç†æˆ–æ— æ‰¹é‡æ±‡æ€»å¯ç”¨")
            logging.info("=================================================")
    except Exception:
        pass
    
    # === å®Œæˆå®éªŒè¿½è¸ªå¹¶ä¸Šä¼ åˆ°æœåŠ¡å™¨ ===
    if experiment_tracker:
        try:
            # ç”Ÿæˆæœ€ç»ˆå®éªŒæŠ¥å‘Š
            final_report = experiment_tracker.generate_experiment_report()
            logging.info(f"å®éªŒæŠ¥å‘Šå·²ç”Ÿæˆ: {len(experiment_tracker.experiment_results)} ä¸ªç»“æœ")
            
            # ä¸Šä¼ å®éªŒæ•°æ®åˆ°æœåŠ¡å™¨ï¼ˆå‚è€ƒcodespace/main.pyçš„ä¸Šä¼ åŠŸèƒ½ï¼‰
            _upload_experiment_to_server(experiment_tracker, final_report, output_dir)
            
            # å®ŒæˆWandBå®éªŒ
            experiment_tracker.finish_experiment()
            logging.info("âœ… å®éªŒè¿½è¸ªå·²å®Œæˆå¹¶ä¸Šä¼ åˆ°æœåŠ¡å™¨")
            
        except Exception as e:
            logging.warning(f"å®Œæˆå®éªŒè¿½è¸ªæ—¶å‡ºé”™: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿè¦å°è¯•å®Œæˆå®éªŒ
            try:
                experiment_tracker.finish_experiment()
            except:
                pass


def _record_batch_results_to_tracker(experiment_tracker: GISExperimentTracker, 
                                    batch_result, experiment_name: str, setting_name: str) -> None:
    """
    å°†æ‰¹å¤„ç†ç»“æœè®°å½•åˆ°å®éªŒè¿½è¸ªå™¨
    
    Args:
        experiment_tracker: å®éªŒè¿½è¸ªå™¨å®ä¾‹
        batch_result: æ‰¹å¤„ç†ç»“æœ
        experiment_name: å®éªŒåç§°
        setting_name: å®éªŒè®¾ç½®åç§°
    """
    try:
        from src.tracking import GISExperimentResult
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_results = len(batch_result.results)
        successful_results = [r for r in batch_result.results if r.success]
        success_rate = len(successful_results) / total_results if total_results > 0 else 0.0
        
        # è®¡ç®—å¹³å‡ç¾è§‚æ€§åˆ†æ•°å’Œæ”¹å–„åˆ†æ•°
        beauty_scores = []
        improvement_scores = []
        dimension_scores_sum = {}
        
        for result in successful_results:
            if hasattr(result, 'evaluation_result') and result.evaluation_result:
                eval_result = result.evaluation_result
                if hasattr(eval_result, 'total_score'):
                    beauty_scores.append(eval_result.total_score)
                if hasattr(eval_result, 'improvement_score'):
                    improvement_scores.append(eval_result.improvement_score)
                if hasattr(eval_result, 'dimension_scores'):
                    for dim, score in eval_result.dimension_scores.items():
                        if dim not in dimension_scores_sum:
                            dimension_scores_sum[dim] = []
                        dimension_scores_sum[dim].append(score)
        
        avg_beauty_score = sum(beauty_scores) / len(beauty_scores) if beauty_scores else 0.0
        avg_improvement_score = sum(improvement_scores) / len(improvement_scores) if improvement_scores else 0.0
        
        # è®¡ç®—å¹³å‡ç»´åº¦åˆ†æ•°
        avg_dimension_scores = {}
        for dim, scores in dimension_scores_sum.items():
            avg_dimension_scores[dim] = sum(scores) / len(scores) if scores else 0.0
        
        # è®¡ç®—æ€»æˆæœ¬å’Œtokenä½¿ç”¨é‡
        total_cost = batch_result.summary.total_cost if hasattr(batch_result.summary, 'total_cost') else 0.0
        total_tokens = sum([r.tokens_used for r in batch_result.results if hasattr(r, 'tokens_used') and r.tokens_used]) or 0
        
        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
        processing_times = [r.processing_time for r in batch_result.results if hasattr(r, 'processing_time') and r.processing_time]
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0.0
        
        # åˆ›å»ºå®éªŒç»“æœè®°å½•
        experiment_result = GISExperimentResult(
            experiment_id=experiment_name,
            timestamp=datetime.now().isoformat(),
            setting_name=setting_name,
            data_version="æ ‡æ³¨æ•°æ®v1",
            evaluation_criteria="5é¡¹è¯„åˆ†æ ‡å‡†",
            beauty_score=avg_beauty_score,
            improvement_score=avg_improvement_score,
            dimension_scores=avg_dimension_scores,
            model_name=experiment_tracker.config.model_name,
            algorithm_version=experiment_tracker.config.algorithm_version,
            prompt_version=experiment_tracker.config.prompt_version,
            api_success_rate=success_rate,
            json_parse_success_rate=success_rate,  # ç®€åŒ–å¤„ç†
            processing_time=avg_processing_time,
            total_tokens=total_tokens,
            total_cost=total_cost,
            is_best_attempt=True  # ä¸»æµç¨‹çš„ç»“æœæ ‡è®°ä¸ºæœ€ä½³å°è¯•
        )
        
        # è®°å½•åˆ°è¿½è¸ªå™¨
        experiment_tracker.log_experiment_result(experiment_result)
        
        # è®¡ç®—å’Œè®°å½•æ”¹å–„æŒ‡æ ‡
        if beauty_scores and improvement_scores:
            improvement_metrics = calculate_gis_improvement_metrics(
                before_scores=beauty_scores,  # ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ˜¯æ²»ç†å‰åˆ†æ•°
                after_scores=[s + i for s, i in zip(beauty_scores, improvement_scores)],
                dimension_improvements=avg_dimension_scores
            )
            experiment_tracker.log_improvement_metrics(improvement_metrics)
        
        # ç”Ÿæˆå¹¶è®°å½•æŒ‡æ ‡æŠ¥å‘Š
        metrics_report = generate_gis_metrics_report(
            experiment_results=[experiment_result],
            setting_name=setting_name
        )
        experiment_tracker.log_metrics_report(metrics_report)
        
        logging.info(f"å®éªŒç»“æœå·²è®°å½•åˆ°è¿½è¸ªå™¨: å¹³å‡ç¾è§‚æ€§åˆ†æ•°={avg_beauty_score:.2f}, æˆåŠŸç‡={success_rate:.2%}")
        
    except Exception as e:
        logging.warning(f"è®°å½•å®éªŒç»“æœåˆ°è¿½è¸ªå™¨å¤±è´¥: {e}")


def _perform_scoring_analysis(selected_files: list, output_dir: str, 
                            save_scoring_details: bool, experiment_tracker, 
                            setting_name: str) -> None:
    """
    æ‰§è¡Œæ²»ç†å‰åè¯„åˆ†å¯¹æ¯”åˆ†æ
    
    Args:
        selected_files: å¾…åˆ†æçš„æ–‡ä»¶åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        save_scoring_details: æ˜¯å¦ä¿å­˜è¯¦ç»†è¯„åˆ†ç»“æœ
        experiment_tracker: å®éªŒè¿½è¸ªå™¨
        setting_name: å®éªŒè®¾ç½®åç§°
    """
    logging.info("ğŸ¯ å¼€å§‹æ‰§è¡Œæ²»ç†å‰åè¯„åˆ†å¯¹æ¯”åˆ†æ...")
    
    scoring_results = []
    successful_analyses = 0
    
    for file_path in selected_files:
        try:
            base_id = file_path.stem.replace('_zlq', '').replace('_zlh', '')
            treated_file = Path(output_dir) / f"{base_id}_zlh.json"
            
            # æ£€æŸ¥æ²»ç†åæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not treated_file.exists():
                logging.warning(f"æœªæ‰¾åˆ°æ²»ç†åæ–‡ä»¶ï¼Œè·³è¿‡è¯„åˆ†åˆ†æ: {treated_file}")
                continue
            
            # åŠ è½½æ²»ç†å‰åæ•°æ®
            from src.data.input_loader import load_gis_data_from_json
            original_gis_data = load_gis_data_from_json(file_path)
            treated_gis_data = load_gis_data_from_json(treated_file)
            
            # å°†GISDataå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆevaluationå‡½æ•°æœŸæœ›å­—å…¸è¾“å…¥ï¼‰
            original_data = {
                'devices': original_gis_data.devices,
                'buildings': original_gis_data.buildings,
                'roads': original_gis_data.roads,
                'rivers': original_gis_data.rivers,
                'boundaries': original_gis_data.boundaries,
                'metadata': original_gis_data.metadata
            }
            treated_data = {
                'devices': treated_gis_data.devices,
                'buildings': treated_gis_data.buildings,
                'roads': treated_gis_data.roads,
                'rivers': treated_gis_data.rivers,
                'boundaries': treated_gis_data.boundaries,
                'metadata': treated_gis_data.metadata
            }
            
            # æ‰§è¡Œè¯„åˆ†å¯¹æ¯”
            score_result = calculate_beauty_score(original_data, treated_data)
            
            # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
            score_result['file_info'] = {
                'base_id': base_id,
                'original_file': str(file_path),
                'treated_file': str(treated_file)
            }
            
            scoring_results.append(score_result)
            successful_analyses += 1
            
            # è®°å½•åˆ°å®éªŒè¿½è¸ªå™¨
            if experiment_tracker:
                _record_scoring_to_tracker(experiment_tracker, score_result, base_id, setting_name)
            
            # è¾“å‡ºè¯„åˆ†ç»“æœ
            original_score = score_result['original_score']
            treated_score = score_result['treated_score']
            
            logging.info(f"ğŸ“Š å°åŒº {base_id} è¯„åˆ†åˆ†æå®Œæˆ:")
            logging.info(f"   æ²»ç†å‰: {original_score['total_score']:.2f}åˆ† ({original_score['level']})")
            logging.info(f"   æ²»ç†å: {treated_score['total_score']:.2f}åˆ† ({treated_score['level']})")
            
            improvement = treated_score['total_score'] - original_score['total_score']
            improvement_pct = (improvement / max(original_score['total_score'], 0.1)) * 100
            logging.info(f"   æ”¹å–„åº¦: {improvement:+.2f}åˆ† ({improvement_pct:+.1f}%)")
            
            # ä¿å­˜è¯¦ç»†è¯„åˆ†ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if save_scoring_details:
                result_json_path = Path(output_dir) / f"{base_id}_è¯„åˆ†è¯¦æƒ….json"
                with open(result_json_path, 'w', encoding='utf-8') as f:
                    import json
                    json.dump(score_result, f, ensure_ascii=False, indent=2)
                logging.info(f"   è¯¦ç»†è¯„åˆ†å·²ä¿å­˜: {result_json_path}")
                
        except Exception as e:
            logging.error(f"è¯„åˆ†åˆ†æå¤±è´¥ {file_path}: {e}")
            continue
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    if scoring_results:
        _generate_scoring_summary(scoring_results, output_dir, save_scoring_details)
        logging.info(f"âœ… è¯„åˆ†åˆ†æå®Œæˆ: æˆåŠŸåˆ†æ {successful_analyses}/{len(selected_files)} ä¸ªå°åŒº")
    else:
        logging.warning("âŒ è¯„åˆ†åˆ†æå¤±è´¥: æ²¡æœ‰æˆåŠŸåˆ†æçš„å°åŒº")


def _record_scoring_to_tracker(experiment_tracker, score_result: dict, 
                              base_id: str, setting_name: str) -> None:
    """
    å°†è¯„åˆ†ç»“æœè®°å½•åˆ°å®éªŒè¿½è¸ªå™¨
    """
    try:
        from src.tracking import GISExperimentResult
        
        original_score = score_result['original_score']
        treated_score = score_result['treated_score']
        improvement = treated_score['total_score'] - original_score['total_score']
        
        # åˆ›å»ºå®éªŒç»“æœè®°å½•
        experiment_result = GISExperimentResult(
            experiment_id=f"scoring_{base_id}",
            timestamp=datetime.now().isoformat(),
            setting_name=setting_name,
            data_version="æ ‡æ³¨æ•°æ®v1",
            evaluation_criteria="5é¡¹è¯„åˆ†æ ‡å‡†",
            beauty_score=treated_score['total_score'],
            improvement_score=improvement,
            dimension_scores={
                'overhead': treated_score.get('overhead', {}).get('total_score', 0),
                'cable_lines': treated_score.get('cable_lines', {}).get('total_score', 0),
                'branch_boxes': treated_score.get('branch_boxes', {}).get('total_score', 0),
                'access_points': treated_score.get('access_points', {}).get('total_score', 0),
                'meter_boxes': treated_score.get('meter_boxes', {}).get('total_score', 0)
            },
            model_name="scoring_analysis",
            algorithm_version="v1.0",
            prompt_version="v1.0",
            api_success_rate=1.0,
            json_parse_success_rate=1.0,
            processing_time=0.0,
            total_tokens=0,
            total_cost=0.0,
            is_best_attempt=True
        )
        
        experiment_tracker.log_experiment_result(experiment_result)
        
    except Exception as e:
        logging.warning(f"è®°å½•è¯„åˆ†ç»“æœåˆ°è¿½è¸ªå™¨å¤±è´¥: {e}")


def _generate_scoring_summary(scoring_results: list, output_dir: str, 
                            save_details: bool) -> None:
    """
    ç”Ÿæˆè¯„åˆ†åˆ†ææ±‡æ€»æŠ¥å‘Š
    """
    try:
        total_count = len(scoring_results)
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        original_scores = [r['original_score']['total_score'] for r in scoring_results]
        treated_scores = [r['treated_score']['total_score'] for r in scoring_results]
        improvements = [t - o for o, t in zip(original_scores, treated_scores)]
        
        avg_original = sum(original_scores) / total_count
        avg_treated = sum(treated_scores) / total_count
        avg_improvement = sum(improvements) / total_count
        avg_improvement_pct = (avg_improvement / max(avg_original, 0.1)) * 100
        
        # æ”¹å–„ç»Ÿè®¡
        improved_count = sum(1 for imp in improvements if imp > 0)
        unchanged_count = sum(1 for imp in improvements if imp == 0)
        degraded_count = sum(1 for imp in improvements if imp < 0)
        
        # è¾“å‡ºæ±‡æ€»æ—¥å¿—
        logging.info("ğŸ“ˆ è¯„åˆ†åˆ†ææ±‡æ€»æŠ¥å‘Š:")
        logging.info(f"   åˆ†æå°åŒºæ•°é‡: {total_count}")
        logging.info(f"   å¹³å‡æ²»ç†å‰åˆ†æ•°: {avg_original:.2f}åˆ†")
        logging.info(f"   å¹³å‡æ²»ç†ååˆ†æ•°: {avg_treated:.2f}åˆ†")
        logging.info(f"   å¹³å‡æ”¹å–„åº¦: {avg_improvement:+.2f}åˆ† ({avg_improvement_pct:+.1f}%)")
        logging.info(f"   æ”¹å–„å°åŒº: {improved_count} ({improved_count/total_count*100:.1f}%)")
        logging.info(f"   æ— å˜åŒ–å°åŒº: {unchanged_count} ({unchanged_count/total_count*100:.1f}%)")
        logging.info(f"   é€€åŒ–å°åŒº: {degraded_count} ({degraded_count/total_count*100:.1f}%)")
        
        # ä¿å­˜æ±‡æ€»æŠ¥å‘Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if save_details:
            summary_data = {
                'analysis_summary': {
                    'total_count': total_count,
                    'avg_original_score': avg_original,
                    'avg_treated_score': avg_treated,
                    'avg_improvement': avg_improvement,
                    'avg_improvement_percentage': avg_improvement_pct,
                    'improved_count': improved_count,
                    'unchanged_count': unchanged_count,
                    'degraded_count': degraded_count
                },
                'detailed_results': scoring_results,
                'timestamp': datetime.now().isoformat()
            }
            
            summary_path = Path(output_dir) / "è¯„åˆ†åˆ†ææ±‡æ€»æŠ¥å‘Š.json"
            with open(summary_path, 'w', encoding='utf-8') as f:
                import json
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            logging.info(f"   æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
            
    except Exception as e:
        logging.error(f"ç”Ÿæˆè¯„åˆ†æ±‡æ€»æŠ¥å‘Šå¤±è´¥: {e}")


def _upload_experiment_to_server(experiment_tracker: GISExperimentTracker, 
                                final_report: dict, output_dir: str) -> None:
    """
    ä¸Šä¼ å®éªŒæ•°æ®åˆ°æœåŠ¡å™¨ï¼ˆå‚è€ƒcodespace/main.pyçš„ä¸Šä¼ åŠŸèƒ½ï¼‰
    
    Args:
        experiment_tracker: å®éªŒè¿½è¸ªå™¨å®ä¾‹
        final_report: æœ€ç»ˆå®éªŒæŠ¥å‘Š
        output_dir: è¾“å‡ºç›®å½•
    """
    try:
        import wandb
        import os
        from pathlib import Path
        
        logger = logging.getLogger(__name__)
        
        # ç¡®ä¿WandBå·²åˆå§‹åŒ–
        if not experiment_tracker.wandb_run:
            logger.warning("WandBæœªåˆå§‹åŒ–ï¼Œè·³è¿‡ä¸Šä¼ ")
            return
        
        # è®°å½•æœ€ç»ˆå®éªŒæ±‡æ€»åˆ°WandB
        if experiment_tracker.wandb_run and hasattr(experiment_tracker.wandb_run, 'log'):
            try:
                # è®°å½•å®éªŒæ±‡æ€»ç»Ÿè®¡
                experiment_summary = {
                    'total_experiment_results': len(experiment_tracker.experiment_results),
                    'total_api_calls': len(experiment_tracker.api_calls),
                    'experiment_duration': final_report.get('experiment_metadata', {}).get('duration', 0),
                    'setting_name': experiment_tracker.config.setting_name,
                    'model_name': experiment_tracker.config.model_name
                }
                
                # å¦‚æœæœ‰æ€§èƒ½ç»Ÿè®¡ï¼Œæ·»åŠ åˆ°æ±‡æ€»ä¸­
                if 'performance_statistics' in final_report:
                    perf_stats = final_report['performance_statistics']
                    if 'beauty_score' in perf_stats:
                        experiment_summary.update({
                            'avg_beauty_score': perf_stats['beauty_score'].get('mean', 0),
                            'max_beauty_score': perf_stats['beauty_score'].get('max', 0),
                            'min_beauty_score': perf_stats['beauty_score'].get('min', 0)
                        })
                    if 'improvement_score' in perf_stats:
                        experiment_summary.update({
                            'avg_improvement_score': perf_stats['improvement_score'].get('mean', 0),
                            'max_improvement_score': perf_stats['improvement_score'].get('max', 0)
                        })
                
                # è®°å½•åˆ°WandB
                wandb.log({
                    'experiment_final_summary': experiment_summary,
                    'final_report': final_report
                })
                
                logger.info(f"âœ… å®éªŒæ•°æ®å·²ä¸Šä¼ åˆ°WandBæœåŠ¡å™¨")
                logger.info(f"ğŸ“Š å®éªŒç»“æœæ•°é‡: {experiment_summary['total_experiment_results']}")
                logger.info(f"ğŸ”— APIè°ƒç”¨æ¬¡æ•°: {experiment_summary['total_api_calls']}")
                
                # æ‰“å°WandBè®¿é—®é“¾æ¥
                if hasattr(experiment_tracker, 'wandb_run') and experiment_tracker.wandb_run:
                    wandb_url = experiment_tracker.wandb_run.url
                    if wandb_url:
                        logger.info("="*60)
                        logger.info("ğŸŒ WandBå®éªŒè¿½è¸ªé“¾æ¥")
                        logger.info("="*60)
                        logger.info(f"ğŸ“Š å®éªŒè®¿é—®é“¾æ¥: {wandb_url}")
                        logger.info(f"ğŸ“ˆ æ‚¨å¯ä»¥é€šè¿‡ä¸Šè¿°é“¾æ¥æŸ¥çœ‹è¯¦ç»†çš„å®éªŒç»“æœå’Œå¯è§†åŒ–å›¾è¡¨")
                        logger.info(f"ğŸ” åŒ…å«ï¼šæ¨¡å‹è°ƒç”¨è®°å½•ã€è¯„åˆ†è¯¦æƒ…ã€æˆæœ¬ç»Ÿè®¡ã€æ€§èƒ½æŒ‡æ ‡ç­‰")
                        logger.info("="*60)
                    else:
                        logger.info(f"ğŸŒ WandBå®éªŒID: {experiment_tracker.wandb_run.id}")
                        logger.info(f"ğŸ“ˆ è¯·è®¿é—® https://wandb.ai æŸ¥çœ‹å®éªŒç»“æœ")
                
            except Exception as e:
                logger.warning(f"ä¸Šä¼ å®éªŒæ•°æ®åˆ°WandBå¤±è´¥: {e}")
        
        # ä¿å­˜æœ¬åœ°å®éªŒæ•°æ®å¤‡ä»½
        try:
            experiment_data_dir = Path(output_dir) / 'experiment_data'
            experiment_data_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜å®éªŒæŠ¥å‘Š
            report_path = experiment_data_dir / 'final_experiment_report.json'
            with open(report_path, 'w', encoding='utf-8') as f:
                import json
                json.dump(final_report, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜å®éªŒé…ç½®
            config_path = experiment_data_dir / 'experiment_config.json'
            with open(config_path, 'w', encoding='utf-8') as f:
                import json
                from dataclasses import asdict
                json.dump(asdict(experiment_tracker.config), f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“ å®éªŒæ•°æ®æœ¬åœ°å¤‡ä»½å·²ä¿å­˜: {experiment_data_dir}")
            
        except Exception as e:
            logger.warning(f"ä¿å­˜æœ¬åœ°å®éªŒæ•°æ®å¤‡ä»½å¤±è´¥: {e}")
        
        # æ¸…ç†ä»£ç†è®¾ç½®
        try:
            del os.environ['HTTP_PROXY']
            del os.environ['HTTPS_PROXY']
        except KeyError:
            pass
            
    except Exception as e:
        logger.error(f"ä¸Šä¼ å®éªŒæ•°æ®åˆ°æœåŠ¡å™¨å¤±è´¥: {e}")
        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹


def main() -> None:
    """
    ä¸»å‡½æ•°
    """
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parse_arguments()
        
        # è®¾ç½®æ—¥å¿—
        setup_logging(args.log_level)
        logger = logging.getLogger(__name__)
        
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # è§£ææ¨¡å‹åˆ—è¡¨
        models = [m.strip() for m in args.models.split(',')]
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ç”µç½‘å°åŒºç¾åŒ–æ²»ç†ä¸æ‰“åˆ†ç³»ç»Ÿå¯åŠ¨")
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {models}")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        # ç»Ÿä¸€å¤„ç†ï¼šè‡ªåŠ¨è¯†åˆ«å•æ–‡ä»¶æˆ–æ‰¹é‡ç›®å½•
        process_areas(
            input_path=args.input,
            output_dir=str(output_dir),
            models=models,
            config=config,
            enable_tracking=args.enable_tracking,
            experiment_name=args.experiment_name,
            setting_name=args.setting_name,
            enable_scoring=args.enable_scoring,
            scoring_only=args.scoring_only,
            save_scoring_details=args.save_scoring_details,
            resume_run_id=args.resume_run_id,
            resume_mode=args.resume_mode
        )
        
        logger.info("å¤„ç†å®Œæˆ")
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")
        logging.error(f"ç³»ç»Ÿé”™è¯¯: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    sys.argv = [
        "main.py",
        "--output", "D:\\work\\resGIS_qwen",
        "--enable-scoring",
        "--enable-tracking",
        "--save-scoring-details",
        "--resume-run-id", "eyunvwgr",
        "D:\\work\\dy_gis_mgx\\æ ‡æ³¨æ•°æ®ç›®å½•\\æœ‰å¯¹åº”å…³ç³»çš„æ ‡æ³¨ç»“æœæ•°æ®"
    ]
    main()
#!/usr/bin/env python3
"""
ç”µç½‘å°åŒºç¾åŒ–æ²»ç†ä¸æ‰“åˆ†ç³»ç»Ÿ - ä¸»ç¨‹åºå…¥å£

è¿™æ˜¯ç³»ç»Ÿçš„æ­£å¼ä¸»å…¥å£ç¨‹åºï¼Œè´Ÿè´£ï¼š
1. è§£æå‘½ä»¤è¡Œå‚æ•°
2. åˆå§‹åŒ–ç³»ç»Ÿé…ç½®
3. è°ƒç”¨æ ¸å¿ƒå¤„ç†æ¨¡å—
4. è¾“å‡ºç»“æœ

ä½œè€…: Yilong Ju
æ—¥æœŸ: 2025å¹´8æœˆ4æ—¥
ç‰ˆæœ¬: Phase 1 åŸºç¡€æ¡†æ¶
"""

import sys
import os
from datetime import datetime
from pathlib import Path
import argparse
import logging

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent.absolute()
src_dir = current_dir / 'src'
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from src.data.device_position_compare import compare_device_positions

def setup_logging(log_level: str = "INFO") -> None:
    """
    é…ç½®æ—¥å¿—ç³»ç»Ÿ
    
    Args:
        log_level: æ—¥å¿—çº§åˆ« (DEBUG, INFO, WARNING, ERROR)
        """
    os.makedirs('logs', exist_ok=True)
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',
        handlers=[
            logging.StreamHandler(),
            # TODO: æ·»åŠ æ–‡ä»¶æ—¥å¿—å¤„ç†å™¨
            logging.FileHandler(filename=f'logs/system_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
        ]
    )

def parse_arguments() -> argparse.Namespace:
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        è§£æåçš„å‚æ•°å¯¹è±¡
    """
    parser = argparse.ArgumentParser(
        description="ç”µç½‘å°åŒºç¾åŒ–æ²»ç†ä¸æ‰“åˆ†ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py data/area_001.json --output results/              # å•æ–‡ä»¶å¤„ç†
  python main.py data/batch/ --models qwen,openai --output results/ # æ‰¹é‡å¤„ç†
        """
    )
    
    # è¾“å…¥é€‰é¡¹
    parser.add_argument(
        'input', 
        type=str,
        help='å°åŒºæ•°æ®æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„ (å•æ–‡ä»¶æˆ–æ‰¹é‡ç›®å½•)'
    )
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='./results',
        help='ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: ./results)'
    )
    
    # æ¨¡å‹é€‰é¡¹
    parser.add_argument(
        '--models', '-m',
        type=str,
        default='qwen',
        help='ä½¿ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš” (é»˜è®¤: qwen)'
    )
    
    # é…ç½®é€‰é¡¹
    parser.add_argument(
        '--config', '-c',
        type=str,
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (YAML/JSONæ ¼å¼)'
    )
    
    # å¹¶è¡Œå¤„ç†é€‰é¡¹
    parser.add_argument(
        '--workers', '-w',
        type=int,
        default=4,
        help='å¹¶è¡Œå¤„ç†çš„workeræ•°é‡ (é»˜è®¤: 4)'
    )
    
    # æ—¥å¿—é€‰é¡¹
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)'
    )
    
    # å®éªŒè¿½è¸ªé€‰é¡¹
    parser.add_argument(
        '--enable-wandb',
        action='store_true',
        help='å¯ç”¨WandBå®éªŒè¿½è¸ª'
    )
    
    return parser.parse_args()

def load_config(config_path: str = None) -> dict:
    """
    åŠ è½½ç³»ç»Ÿé…ç½®
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        é…ç½®å­—å…¸
    """
    # TODO: å®ç°é…ç½®æ–‡ä»¶åŠ è½½
    # 1. åŠ è½½é»˜è®¤é…ç½®
    # 2. åŠ è½½ç”¨æˆ·é…ç½®æ–‡ä»¶ (å¦‚æœæä¾›)
    # 3. åŠ è½½ç¯å¢ƒå˜é‡é…ç½®
    # 4. åˆå¹¶é…ç½®
    
    default_config = {
        'models': {
            'qwen': {
                'api_key': os.getenv('QWEN_API_KEY'),
                'model_name': 'qwen-vl-max-2025-04-08',
                'timeout': 30
            }
            # TODO: æ·»åŠ å…¶ä»–æ¨¡å‹é…ç½®
        },
        'processing': {
            'max_workers': 4,
            'timeout': 300,
            'retry_count': 3
        },
        'output': {
            'save_images': True,
            'save_logs': True,
            'format': 'json'
        }
    }
    
    return default_config

def process_areas(input_path: str, output_dir: str, models: list, config: dict) -> None:
    """
    å¤„ç†å°åŒºæ•°æ® (è‡ªåŠ¨è¯†åˆ«å•æ–‡ä»¶æˆ–æ‰¹é‡ç›®å½•)
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        models: æ¨¡å‹åˆ—è¡¨
        config: é…ç½®å­—å…¸
    """
    from pathlib import Path
    
    input_path_obj = Path(input_path)
    
    if not input_path_obj.exists():
        raise FileNotFoundError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
    
    if input_path_obj.is_file():
        # å•æ–‡ä»¶å¤„ç† (batch size = 1)
        print(f"ğŸ¯ å¤„ç†å•ä¸ªå°åŒºæ–‡ä»¶: {input_path}")
        input_files = [input_path_obj]
    elif input_path_obj.is_dir():
        # æ‰¹é‡å¤„ç† (æ‰«æç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶)
        input_files = list(input_path_obj.glob("*.json"))[:20]  # TODO æµ‹è¯•æ‰¹é‡è¿è¡Œå‰äº”ä¸ªæ–‡ä»¶
        print(f"ğŸ¯ æ‰¹é‡å¤„ç†å°åŒºç›®å½•: {input_path}")
        print(f"ğŸ“‹ å‘ç° {len(input_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    else:
        raise ValueError(f"è¾“å…¥è·¯å¾„æ—¢ä¸æ˜¯æ–‡ä»¶ä¹Ÿä¸æ˜¯ç›®å½•: {input_path}")
    
    if not input_files:
        raise ValueError("æœªæ‰¾åˆ°ä»»ä½•.jsonæ•°æ®æ–‡ä»¶")
    
    print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {models}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ”„ å¹¶è¡Œæ•°: {config['processing']['max_workers']}")
    
    # TODO: å®ç°ç»Ÿä¸€å¤„ç†é€»è¾‘
    # 1. åŠ è½½æ‰€æœ‰GISæ•°æ®æ–‡ä»¶
    # 2. åˆ›å»ºBatchInput (å³ä½¿åªæœ‰1ä¸ªæ–‡ä»¶)
    # 3. è°ƒç”¨core.pipeline.process_batch
    # 4. ä¿å­˜ç»“æœ
    
    from core.pipeline import process_batch
    from core.data_types import BatchInput, ImageInput, GISData
    from data.input_loader import load_gis_data_from_json
    from data.output_saver import save_batch_results
    # 
    # # åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶å¹¶åˆ›å»ºImageInputåˆ—è¡¨
    # ä»…é€‰æ‹©æœªæ²»ç†æ–‡ä»¶ï¼šä¼˜å…ˆ _zlq.jsonï¼Œå…¶æ¬¡æ— åç¼€ï¼Œæ’é™¤ _zlh.json
    def _base_id(p):
        name = p.stem
        return name.replace('_zlq', '').replace('_zlh', '')

    selected_map = {}
    for p in input_files:
        if p.name.endswith('_zlh.json'):
            continue  # æ’é™¤å·²æ²»ç†æ–‡ä»¶ï¼Œé¿å…é‡å¤
        b = _base_id(p)
        # å¦‚æœå·²æœ‰å€™é€‰ä¸”ä¸æ˜¯_zlqï¼Œä¼˜å…ˆç”¨_zlqæ›¿æ¢
        if b in selected_map:
            # ç°æœ‰å€™é€‰æ˜¯å¦ä¸º_zlq
            if not selected_map[b].name.endswith('_zlq.json') and p.name.endswith('_zlq.json'):
                selected_map[b] = p
        else:
            selected_map[b] = p

    selected_files = list(selected_map.values())
    logging.info(f"ç­›é€‰åå¾…å¤„ç†æ–‡ä»¶æ•°: {len(selected_files)}ï¼ˆæ’é™¤_zlhå·²æ²»ç†æ–‡ä»¶ï¼‰")

    # åŸºäºè¾“å‡ºç›®å½•ç°çŠ¶åšäºŒæ¬¡ç­›é€‰ï¼šè‹¥åŒæ—¶å­˜åœ¨æ²»ç†åJSONä¸å¯¹æ¯”å›¾ï¼Œåˆ™å½»åº•è·³è¿‡
    files_to_process = []
    skipped_by_existing = []
    skipped_by_existing_ids = []
    for file_path in selected_files:
        base_id = file_path.stem.replace('_zlq', '').replace('_zlh', '')
        treated_json = os.path.join(output_dir, f"{base_id}_zlh.json")
        compare_png = os.path.join(output_dir, f"{base_id}_result.png")
        if os.path.exists(treated_json) and os.path.exists(compare_png):
            skipped_by_existing.append(file_path)
            skipped_by_existing_ids.append(base_id)
        else:
            files_to_process.append(file_path)

    # å»¶åæ‰“å°â€œå·²è·³è¿‡â€æ˜ç»†åˆ°æ‰¹å¤„ç†å¼€å§‹ä¹‹åï¼Œä»¥ä¾¿pipelineæ—¥å¿—å…ˆå‡ºç°

    inputs = []
    for file_path in files_to_process:
        gis_data = load_gis_data_from_json(file_path)
        image_input = ImageInput(gis_data=gis_data, input_id=file_path.stem)
        inputs.append(image_input)
    # 
    # # åˆ›å»ºBatchInput
    batch_input = BatchInput(
        inputs=inputs,
        config=config,
        batch_id=f"main_{len(inputs)}files"
    )
    # 
    # # è°ƒç”¨æ‰¹å¤„ç†
    if inputs:
        batch_result = process_batch(
            batch_input, 
            models=models,
            max_workers=config['processing']['max_workers']
        )
        # ä¿å­˜ç»“æœåˆ°è¾“å‡ºç›®å½•
        save_batch_results(batch_result, output_dir)
    else:
        logging.info("æœ¬æ¬¡æ— éœ€æ²»ç†ï¼šå…¨éƒ¨å°åŒºå‡å·²å­˜åœ¨æ²»ç†ç»“æœä¸å¯¹æ¯”å›¾")

    # ç°åœ¨è¾“å‡ºå·²è·³è¿‡çš„å°åŒºæ˜ç»†ä¸ç»Ÿè®¡ï¼ˆä¿è¯å‡ºç°åœ¨pipelineå¼€å§‹æ—¥å¿—ä¹‹åï¼‰
    if skipped_by_existing_ids:
        for base_id in skipped_by_existing_ids:
            logging.info(f"æ£€æµ‹åˆ°å·²æ²»ç†ä¸”å·²æœ‰å¯¹æ¯”å›¾ï¼Œè·³è¿‡: {base_id}")
        logging.info(f"å·²è·³è¿‡ {len(skipped_by_existing_ids)} ä¸ªå°åŒºï¼ˆå‡å·²å­˜åœ¨JSONä¸å¯¹æ¯”å›¾ï¼‰")
 
    # === ç”Ÿæˆè®¾å¤‡ä½ç½®å¯¹æ¯”å›¾ ===
    # è§„åˆ™ï¼šåªè¦æœ‰æ²»ç†åJSONä½†ç¼ºå°‘å¯¹æ¯”å›¾ï¼Œå°±è¡¥é½å¯¹æ¯”å›¾ï¼›å¦åˆ™è·³è¿‡
    for file1 in selected_files:
        base_id = file1.stem.replace('_zlq', '').replace('_zlh', '')
        file2 = os.path.join(output_dir, base_id + '_zlh.json')
        out_img_path = os.path.join(output_dir, base_id + '_result.png')
        if not Path(file2).exists():
            logging.warning(f"æœªæ‰¾åˆ°æ²»ç†åjsonæ–‡ä»¶ï¼Œæ— æ³•ç”Ÿæˆå¯¹æ¯”å›¾: {file2}")
            continue
        if Path(out_img_path).exists():
            logging.info(f"å¯¹æ¯”å›¾å·²å­˜åœ¨ï¼Œè·³è¿‡ç”Ÿæˆ: {out_img_path}")
            continue
        # è¯»å–æ²»ç†ç»“æœï¼Œè‹¥æ— è®¾å¤‡åˆ™è·³è¿‡ç”Ÿæˆå¯¹æ¯”å›¾
        try:
            import json
            with open(file2, 'r', encoding='utf-8') as f:
                treated = json.load(f)
            anns = treated.get('annotations', []) or []
            if len(anns) == 0:
                logging.warning(f"æ²»ç†åæ— è®¾å¤‡ï¼ˆannotationsç©ºï¼‰ï¼Œè·³è¿‡å¯¹æ¯”å›¾: {file2}")
                continue
        except Exception as e:
            logging.warning(f"è¯»å–æ²»ç†åç»“æœå¤±è´¥ï¼Œè·³è¿‡å¯¹æ¯”å›¾: {file2}ï¼ŒåŸå› : {e}")
            continue
        compare_device_positions(file1, file2, out_img_path)

    # === æœ€ç»ˆæ±‡æ€»æ—¥å¿—ï¼ˆæ”¾åœ¨æµç¨‹æœ€æœ«ï¼Œé¿å…è¢«åç»­æ—¥å¿—æ·¹æ²¡ï¼‰ ===
    try:
        if 'batch_result' in locals() and batch_result and getattr(batch_result, 'summary', None):
            logging.info("================ æœ€ç»ˆæ±‡æ€»ï¼ˆæµç¨‹ç»“æŸï¼‰ ================")
            logging.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼Œæ€»æˆæœ¬: ${batch_result.summary.total_cost:.4f}")
            logging.info(f"æˆåŠŸç‡: {batch_result.summary.success_rate:.1f}%")
            run_id = getattr(batch_result, 'wandb_run_id', None)
            if run_id:
                logging.info(f"WandBè¿è¡ŒID: {run_id}")
            logging.info("=================================================")
        else:
            logging.info("================ æœ€ç»ˆæ±‡æ€»ï¼ˆæµç¨‹ç»“æŸï¼‰ ================")
            logging.info("æœ¬æ¬¡æœªæ‰§è¡Œæ²»ç†æˆ–æ— æ‰¹é‡æ±‡æ€»å¯ç”¨")
            logging.info("=================================================")
    except Exception:
        pass
 

def main() -> None:
    """
    ä¸»å‡½æ•°
    """
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parse_arguments()
        
        # è®¾ç½®æ—¥å¿—
        setup_logging(args.log_level)
        logger = logging.getLogger(__name__)
        
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # è§£ææ¨¡å‹åˆ—è¡¨
        models = [m.strip() for m in args.models.split(',')]
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ç”µç½‘å°åŒºç¾åŒ–æ²»ç†ä¸æ‰“åˆ†ç³»ç»Ÿå¯åŠ¨")
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {models}")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        # ç»Ÿä¸€å¤„ç†ï¼šè‡ªåŠ¨è¯†åˆ«å•æ–‡ä»¶æˆ–æ‰¹é‡ç›®å½•
        process_areas(args.input, str(output_dir), models, config)
        
        logger.info("å¤„ç†å®Œæˆ")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")
        logging.error(f"ç³»ç»Ÿé”™è¯¯: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    sys.argv = [
        "main.py",
        "--output", "D:\\work\\resGIS",
        "D:\\work\\dy_gis_mgx\\æ ‡æ³¨æ•°æ®ç›®å½•\\æœ‰å¯¹åº”å…³ç³»çš„æ ‡æ³¨ç»“æœæ•°æ®"
    ]
    main()